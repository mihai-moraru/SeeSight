# SeeSight

**On-device AI vision for iOS â€” powered by Apple's [FastVLM](https://github.com/apple/ml-fastvlm) and Flutter.**

SeeSight is an open-source Flutter iOS app that runs a Vision-Language Model entirely on-device using Apple's MLX framework and Neural Engine. Point your camera at anything and get instant AI-powered descriptions, text recognition, object counting, and more â€” with zero cloud dependency.

<p align="center">
  <img src="assets/logo.png" width="120" alt="SeeSight logo" />
</p>

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| ğŸ¥ **Live Mode** | Continuous real-time camera analysis with adaptive frame rate |
| ğŸ“¸ **Photo Mode** | Capture a single frame and analyze it |
| âš¡ **TTFT Display** | See time-to-first-token for every inference |
| ğŸ”’ **100% On-Device** | No network calls â€” all processing stays on your iPhone |
| ğŸ¨ **Glassmorphic UI** | Modern frosted-glass design with smooth animations |
| ğŸ’¬ **Custom Prompts** | Quick prompt pills + editable prompt field |
| ğŸŒ“ **Theming** | Auto / Light / Dark mode |
| ğŸ“· **Camera Selection** | Front or back camera with live switching |
| ğŸ“ **Markdown Responses** | AI responses rendered with full Markdown support |

### Built-in Quick Prompts

- **Describe** â€” General scene description
- **Count** â€” Object counting
- **Read Text** â€” OCR / text recognition
- **Colors** â€” Dominant color detection
- **Emotion** â€” Facial expression analysis

---

## ğŸ“± Requirements

| Requirement | Minimum |
|-------------|---------|
| **iOS** | 18.2+ (required for MLX framework) |
| **Device** | iPhone with A14 Bionic or later (Neural Engine) |
| **Xcode** | 16.0+ |
| **Flutter** | 3.2.0+ |
| **Disk Space** | ~1 GB for the 0.5B model (see [Model Options](#-model-options)) |

---

## ğŸš€ Getting Started

### 1. Clone the repository

```bash
git clone https://github.com/anthropics/seesight.git
cd seesight
```

### 2. Download a FastVLM model

```bash
chmod +x get_pretrained_model.sh
./get_pretrained_model.sh --model 0.5b    # Recommended for mobile
```

See [Model Options](#-model-options) for all available sizes.

### 3. Install dependencies

```bash
flutter pub get
```

### 4. Add the model to Xcode

1. Open `ios/Runner.xcworkspace` in Xcode
2. Right-click the **Runner** folder â†’ **Add Files to "Runner"â€¦**
3. Select the `ios/Runner/model` folder
4. Enable **"Create folder references"** and add to the **Runner** target

### 5. Add Swift Package dependencies

In Xcode â†’ **File â†’ Add Package Dependenciesâ€¦** and add:

| Package | URL | Version |
|---------|-----|---------|
| **mlx-swift** | `https://github.com/ml-explore/mlx-swift.git` | `0.21.2+` |
| **mlx-swift-lm** | `https://github.com/ml-explore/mlx-swift-lm.git` | `0.21.2+` |

From **mlx-swift-lm**, add the products **MLXLMCommon** and **MLXVLM** to the Runner target.

### 6. Configure build settings

In Xcode, select the **Runner** target and verify:

| Setting | Value |
|---------|-------|
| iOS Deployment Target | `18.2` |
| Swift Language Version | `5.0` |
| Build Active Architecture Only (Release) | `Yes` |

### 7. Build and run

```bash
flutter run --release
```

> **Note:** Use a physical device â€” the Simulator doesn't support Neural Engine acceleration.

---

## ğŸ§  Model Options

Three pre-exported MLX models are available from [Apple's CDN](https://github.com/apple/ml-fastvlm):

| Model | Params | Quantization | Size (approx.) | Best For |
|-------|--------|-------------|-----------------|----------|
| **0.5B** | 0.5 billion | FP16 | ~1 GB | Real-time on any device |
| **1.5B** | 1.5 billion | INT8 | ~1.5 GB | Balanced speed & accuracy |
| **7B** | 7 billion | INT4 | ~4 GB | Maximum accuracy |

Download any model with the included script:

```bash
./get_pretrained_model.sh --model 0.5b   # Small and fast
./get_pretrained_model.sh --model 1.5b   # Balanced
./get_pretrained_model.sh --model 7b     # Most accurate
```

You can also specify a custom destination:

```bash
./get_pretrained_model.sh --model 1.5b --dest /path/to/output
```

Each model zip contains:

| File | Purpose |
|------|---------|
| `config.json` | Model architecture definition |
| `model.safetensors` | LLM weights |
| `fastvithd.mlpackage/` | CoreML vision encoder |
| `tokenizer.json` + related | Tokenizer vocabulary |
| `preprocessor_config.json` | Image preprocessing config |

---

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Flutter UI                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚HomeScreenâ”‚ â”‚ Settings â”‚ â”‚  Widgets (glass,  â”‚ â”‚
â”‚  â”‚          â”‚ â”‚  Screen  â”‚ â”‚  bottom sheet,    â”‚ â”‚
â”‚  â”‚          â”‚ â”‚          â”‚ â”‚  mode toggle, â€¦)  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚       â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Riverpod Services (camera, VLM, app state) â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚       â”‚  MethodChannel / EventChannel            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚FastVLMPlugin  â”‚â”€â”€â”‚  FastVLMModel          â”‚   â”‚
â”‚  â”‚(bridge)       â”‚  â”‚  (load, generate,      â”‚   â”‚
â”‚  â”‚               â”‚  â”‚   stream)              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  FastVLM / FastViTHD (MLX + CoreML)          â”‚ â”‚
â”‚  â”‚  Apple Neural Engine                          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                  Native iOS                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Communication Flow

```
Camera Frame â†’ Dart (adaptive frame skip) â†’ MethodChannel â†’ Swift Plugin
    â†’ FastVLMModel â†’ MLX Inference (Neural Engine) â†’ Response
    â†’ EventChannel (streaming tokens) â†’ Dart â†’ Markdown UI
```

### Project Structure

```
seesight/
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ main.dart                          # App entry point + theme
â”‚   â”œâ”€â”€ screens/
â”‚   â”‚   â”œâ”€â”€ home_screen.dart               # Main camera + VLM screen
â”‚   â”‚   â””â”€â”€ settings_screen.dart           # Settings, about, easter egg
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ vlm_service.dart               # VLM MethodChannel service (Riverpod)
â”‚   â”‚   â”œâ”€â”€ camera_service.dart            # Camera lifecycle + adaptive frame rate
â”‚   â”‚   â”œâ”€â”€ app_state.dart                 # App state, errors, combined providers
â”‚   â”‚   â””â”€â”€ settings_service.dart          # Theme & camera preferences
â”‚   â””â”€â”€ widgets/
â”‚       â”œâ”€â”€ camera_preview_widget.dart     # Full-screen camera with switch button
â”‚       â”œâ”€â”€ response_bottom_sheet.dart     # Draggable response sheet + prompts
â”‚       â”œâ”€â”€ glass_container.dart           # Reusable glassmorphic container
â”‚       â”œâ”€â”€ mode_toggle.dart               # Live / Photo animated toggle
â”‚       â”œâ”€â”€ status_indicator.dart          # Ready / Processing / Generating pill
â”‚       â””â”€â”€ error_dialog.dart              # Error listener + retry dialogs
â”œâ”€â”€ ios/
â”‚   â””â”€â”€ Runner/
â”‚       â”œâ”€â”€ AppDelegate.swift              # Plugin registration
â”‚       â”œâ”€â”€ FastVLMPlugin.swift            # MethodChannel + EventChannel bridge
â”‚       â”œâ”€â”€ FastVLMModel.swift             # Model loading & inference wrapper
â”‚       â””â”€â”€ FastVLM/                       # Core VLM (ported from Apple)
â”‚           â”œâ”€â”€ FastVLM.swift              # Model architecture + registration
â”‚           â”œâ”€â”€ FastVITHD.swift            # CoreML vision encoder
â”‚           â””â”€â”€ MediaProcessingExtensions.swift  # Image processing utils
â”œâ”€â”€ test/                                  # Widget & unit tests
â”œâ”€â”€ assets/                                # App icon
â”œâ”€â”€ get_pretrained_model.sh                # Model download helper
â”œâ”€â”€ pubspec.yaml
â””â”€â”€ LICENSE
```

### Key Technologies

| Technology | Role |
|------------|------|
| [Flutter](https://flutter.dev/) | UI framework |
| [Riverpod 3.x](https://riverpod.dev/) | State management (`Notifier` API) |
| [MLX Swift](https://github.com/ml-explore/mlx-swift) | On-device ML for Apple Silicon |
| [FastVLM](https://github.com/apple/ml-fastvlm) | Vision-language model architecture |
| CoreML + Neural Engine | Hardware-accelerated vision encoding |

---

## ğŸ§ª Testing

```bash
# Run all tests
flutter test

# Static analysis
flutter analyze
```

---

## ğŸ¤ Contributing

Contributions are welcome! Here's how:

1. **Fork** the repository
2. **Create a branch:** `git checkout -b feature/my-feature`
3. **Make changes** and add tests if applicable
4. **Verify:**
   ```bash
   flutter analyze   # Zero issues required
   flutter test      # All tests passing
   ```
5. **Commit** with a clear message
6. **Open a Pull Request**

### Areas for Contribution

- ğŸŒ **Runtime model download** â€” Download models on-demand instead of bundling *(planned)*
- ğŸ¤– **Custom model URLs** â€” Paste URLs for any compatible FastVLM model *(planned)*
- ğŸ¨ **UI/UX** â€” Animations, accessibility, new themes
- ğŸ“Š **Performance** â€” Frame rate tuning, memory optimization
- ğŸ§ª **Testing** â€” Widget tests, integration tests, golden tests
- ğŸ“– **Docs** â€” Screenshots, tutorials, translations

---

## âš ï¸ Troubleshooting

### Model not loading
- Ensure the `model` folder is added to the Xcode Runner target as a **folder reference**
- Verify the folder appears in **Build Phases â†’ Copy Bundle Resources**
- Check your device runs iOS 18.2+

### Camera not working
- Grant camera permission when prompted
- `Info.plist` must contain `NSCameraUsageDescription`

### Build errors
- Run `cd ios && pod install && cd ..`
- Clean: `flutter clean && flutter pub get`
- Verify SPM packages (mlx-swift, mlx-swift-lm) are resolved in Xcode

### "Module not found" for MLX
- In Xcode, go to **File â†’ Packages â†’ Resolve Package Versions**
- Ensure both `mlx-swift` and `mlx-swift-lm` are added to the Runner target

---

## ğŸ“„ License

This project is licensed under the **MIT License** â€” see [LICENSE](LICENSE) for details.

Files in `ios/Runner/FastVLM/` are derived from [Apple's ml-fastvlm](https://github.com/apple/ml-fastvlm) and are subject to Apple's original license (Copyright Â© 2025 Apple Inc.).

---

## ğŸ“š Citation

If you use FastVLM in your research, please cite:

```bibtex
@inproceedings{fastvlm2025,
  title     = {FastVLM: Efficient Vision Encoding for Vision Language Models},
  author    = {Vasu, Pavan Kumar Anasosalu and Faghri, Fartash and Li, Chun-Liang
               and Koc, Cem and True, Nate and Antony, Albert and Santhanam, Gokul
               and Gabriel, James and Grasch, Peter and Tuzel, Oncel
               and Pouransari, Hadi},
  booktitle = {CVPR},
  year      = {2025}
}
```

---

## ğŸ™ Acknowledgements

- [Apple ML Research](https://github.com/apple/ml-fastvlm) â€” FastVLM model & architecture
- [MLX Swift](https://github.com/ml-explore/mlx-swift) â€” On-device ML framework
- [Flutter](https://flutter.dev/) â€” Cross-platform UI
- [Riverpod](https://riverpod.dev/) â€” Reactive state management
